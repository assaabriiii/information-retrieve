{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval Project Demo\n",
    "\n",
    "This notebook demonstrates the information retrieval project that involves web scraping, data preprocessing, clustering, classification, and visualization of data from two websites: books.toscrape.com and quotes.toscrape.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's import the necessary modules and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import project modules\n",
    "from src.scraper.books_scraper import BooksScraper\n",
    "from src.scraper.quotes_scraper import QuotesScraper\n",
    "from src.preprocessing.text_processor import TextProcessor\n",
    "from src.analysis.clustering import ClusteringAnalyzer\n",
    "from src.analysis.classification import ClassificationAnalyzer\n",
    "from src.visualization.visualizer import Visualizer\n",
    "\n",
    "# Set up matplotlib for inline display\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Create necessary directories\n",
    "directories = [\n",
    "    \"data\",\n",
    "    \"data/books\",\n",
    "    \"data/books/html\",\n",
    "    \"data/quotes\",\n",
    "    \"data/quotes/html\",\n",
    "    \"results\",\n",
    "    \"results/figures\",\n",
    "    \"results/models\"\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    os.makedirs(directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Web Scraping\n",
    "\n",
    "Let's scrape data from the books and quotes websites. We'll scrape 10 pages from each website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of pages to scrape\n",
    "num_pages = 10\n",
    "\n",
    "# Scrape books\n",
    "print(\"=== Scraping Books ===\\n\")\n",
    "books_scraper = BooksScraper()\n",
    "books_data = books_scraper.scrape_pages(num_pages)\n",
    "print(f\"Scraped {len(books_data)} books.\\n\")\n",
    "\n",
    "# Scrape quotes\n",
    "print(\"\\n=== Scraping Quotes ===\\n\")\n",
    "quotes_scraper = QuotesScraper()\n",
    "quotes_data = quotes_scraper.scrape_pages(num_pages)\n",
    "print(f\"Scraped {len(quotes_data)} quotes.\\n\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nSample book data:\")\n",
    "print(books_data[0])\n",
    "\n",
    "print(\"\\nSample quote data:\")\n",
    "print(quotes_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "Now, let's preprocess the scraped data to clean and standardize the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text processor\n",
    "text_processor = TextProcessor()\n",
    "\n",
    "# Preprocess books data\n",
    "print(\"=== Preprocessing Books Data ===\\n\")\n",
    "processed_books = text_processor.preprocess_data(books_data, 'description')\n",
    "print(f\"Preprocessed {len(processed_books)} books.\\n\")\n",
    "\n",
    "# Preprocess quotes data\n",
    "print(\"\\n=== Preprocessing Quotes Data ===\\n\")\n",
    "processed_quotes = text_processor.preprocess_data(quotes_data, 'text')\n",
    "print(f\"Preprocessed {len(processed_quotes)} quotes.\\n\")\n",
    "\n",
    "# Display sample preprocessed data\n",
    "print(\"\\nSample preprocessed book description:\")\n",
    "print(\"Original:\", processed_books[0]['description'])\n",
    "print(\"Processed:\", processed_books[0]['processed_description'])\n",
    "\n",
    "print(\"\\nSample preprocessed quote text:\")\n",
    "print(\"Original:\", processed_quotes[0]['text'])\n",
    "print(\"Processed:\", processed_quotes[0]['processed_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clustering Analysis\n",
    "\n",
    "Let's perform clustering on the books and quotes data to group similar items together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Clustering Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract preprocessed descriptions\n",
    "descriptions = [book['processed_description'] for book in processed_books]\n",
    "\n",
    "# Vectorize descriptions\n",
    "X_books, vectorizer_books = text_processor.vectorize_tfidf(descriptions)\n",
    "\n",
    "# Find optimal number of clusters\n",
    "clustering = ClusteringAnalyzer()\n",
    "optimal_clusters, inertia_values = clustering.get_optimal_clusters(X_books, max_clusters=10)\n",
    "print(f\"Optimal number of clusters for books: {optimal_clusters}\\n\")\n",
    "\n",
    "# Plot elbow curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(2, len(inertia_values) + 2), inertia_values, marker='o')\n",
    "plt.title('Elbow Method for Optimal k (Books)')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.axvline(x=optimal_clusters, color='red', linestyle='--')\n",
    "plt.text(optimal_clusters + 0.1, max(inertia_values) * 0.9, f'Optimal k = {optimal_clusters}')\n",
    "plt.show()\n",
    "\n",
    "# Create clustering analyzer with optimal number of clusters\n",
    "book_clustering = ClusteringAnalyzer(n_clusters=optimal_clusters)\n",
    "\n",
    "# Perform clustering\n",
    "book_clusters = book_clustering.cluster_kmeans(X_books)\n",
    "print(f\"Clustered {len(book_clusters)} books into {len(set(book_clusters))} clusters.\\n\")\n",
    "\n",
    "# Get cluster distribution\n",
    "distribution = book_clustering.get_cluster_distribution()\n",
    "print(\"Cluster distribution:\")\n",
    "for cluster, count in distribution.items():\n",
    "    print(f\"Cluster {cluster}: {count} books\")\n",
    "\n",
    "# Visualize clusters\n",
    "fig = book_clustering.visualize_clusters(X_books, title=\"Book Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Clustering Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract preprocessed quotes\n",
    "quotes_text = [quote['processed_text'] for quote in processed_quotes]\n",
    "\n",
    "# Vectorize quotes\n",
    "X_quotes, vectorizer_quotes = text_processor.vectorize_tfidf(quotes_text)\n",
    "\n",
    "# Find optimal number of clusters\n",
    "clustering = ClusteringAnalyzer()\n",
    "optimal_clusters, inertia_values = clustering.get_optimal_clusters(X_quotes, max_clusters=10)\n",
    "print(f\"Optimal number of clusters for quotes: {optimal_clusters}\\n\")\n",
    "\n",
    "# Plot elbow curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(2, len(inertia_values) + 2), inertia_values, marker='o')\n",
    "plt.title('Elbow Method for Optimal k (Quotes)')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.axvline(x=optimal_clusters, color='red', linestyle='--')\n",
    "plt.text(optimal_clusters + 0.1, max(inertia_values) * 0.9, f'Optimal k = {optimal_clusters}')\n",
    "plt.show()\n",
    "\n",
    "# Create clustering analyzer with optimal number of clusters\n",
    "quote_clustering = ClusteringAnalyzer(n_clusters=optimal_clusters)\n",
    "\n",
    "# Perform clustering\n",
    "quote_clusters = quote_clustering.cluster_kmeans(X_quotes)\n",
    "print(f\"Clustered {len(quote_clusters)} quotes into {len(set(quote_clusters))} clusters.\\n\")\n",
    "\n",
    "# Get cluster distribution\n",
    "distribution = quote_clustering.get_cluster_distribution()\n",
    "print(\"Cluster distribution:\")\n",
    "for cluster, count in distribution.items():\n",
    "    print(f\"Cluster {cluster}: {count} quotes\")\n",
    "\n",
    "# Visualize clusters\n",
    "fig = quote_clustering.visualize_clusters(X_quotes, title=\"Quote Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classification Analysis\n",
    "\n",
    "Now, let's perform classification on the books and quotes data to categorize them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Classifying Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract preprocessed descriptions and categories\n",
    "descriptions = [book['processed_description'] for book in processed_books]\n",
    "categories = [book['category'] for book in processed_books]\n",
    "\n",
    "# Vectorize descriptions\n",
    "X_books, vectorizer_books = text_processor.vectorize_tfidf(descriptions)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_books, categories, test_size=0.2, random_state=42, stratify=categories if len(categories) > 1 else None\n",
    ")\n",
    "\n",
    "# Create classification analyzer\n",
    "book_classifier = ClassificationAnalyzer()\n",
    "\n",
    "# Compare different models\n",
    "book_metrics = book_classifier.compare_models(X_books, categories)\n",
    "print(\"Model comparison:\")\n",
    "for model_name, model_metrics in book_metrics.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    for metric_name, metric_value in model_metrics.items():\n",
    "        print(f\"  {metric_name}: {metric_value:.3f}\")\n",
    "\n",
    "# Train the best model (based on F1 score)\n",
    "best_model = max(book_metrics.items(), key=lambda x: x[1]['f1'])[0]\n",
    "print(f\"\\nBest model: {best_model}\\n\")\n",
    "book_classifier.train(X_train, y_train, model_name=best_model)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_metrics = book_classifier.evaluate(X_test, y_test)\n",
    "print(\"Test set evaluation:\")\n",
    "for metric_name, metric_value in test_metrics.items():\n",
    "    print(f\"  {metric_name}: {metric_value:.3f}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig = book_classifier.plot_confusion_matrix(X_test, y_test, title=\"Book Classification Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Classifying Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract preprocessed quotes and primary tags\n",
    "quotes_text = [quote['processed_text'] for quote in processed_quotes]\n",
    "\n",
    "# Use the first tag as the target class\n",
    "primary_tags = [quote['tags'][0] if quote['tags'] else 'unknown' for quote in processed_quotes]\n",
    "\n",
    "# Vectorize quotes\n",
    "X_quotes, vectorizer_quotes = text_processor.vectorize_tfidf(quotes_text)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_quotes, primary_tags, test_size=0.2, random_state=42, stratify=primary_tags if len(primary_tags) > 1 else None\n",
    ")\n",
    "\n",
    "# Create classification analyzer\n",
    "quote_classifier = ClassificationAnalyzer()\n",
    "\n",
    "# Compare different models\n",
    "quote_metrics = quote_classifier.compare_models(X_quotes, primary_tags)\n",
    "print(\"Model comparison:\")\n",
    "for model_name, model_metrics in quote_metrics.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    for metric_name, metric_value in model_metrics.items():\n",
    "        print(f\"  {metric_name}: {metric_value:.3f}\")\n",
    "\n",
    "# Train the best model (based on F1 score)\n",
    "best_model = max(quote_metrics.items(), key=lambda x: x[1]['f1'])[0]\n",
    "print(f\"\\nBest model: {best_model}\\n\")\n",
    "quote_classifier.train(X_train, y_train, model_name=best_model)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_metrics = quote_classifier.evaluate(X_test, y_test)\n",
    "print(\"Test set evaluation:\")\n",
    "for metric_name, metric_value in test_metrics.items():\n",
    "    print(f\"  {metric_name}: {metric_value:.3f}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig = quote_classifier.plot_confusion_matrix(X_test, y_test, title=\"Quote Classification Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizations\n",
    "\n",
    "Let's create visualizations to better understand the data and analysis results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Word Clouds for Book Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize visualizer\n",
    "visualizer = Visualizer()\n",
    "\n",
    "# Create word clouds for book clusters\n",
    "book_descriptions = [book['description'] for book in processed_books]\n",
    "book_wordclouds = visualizer.create_class_wordclouds(\n",
    "    book_descriptions, book_clusters, title_prefix=\"Book Cluster\"\n",
    ")\n",
    "\n",
    "# Display word clouds\n",
    "for cluster, fig in book_wordclouds.items():\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(fig.axes[0].images[0].get_array())\n",
    "    plt.title(f\"Word Cloud for Book Cluster {cluster}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Word Clouds for Quote Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word clouds for quote clusters\n",
    "quote_texts = [quote['text'] for quote in processed_quotes]\n",
    "quote_wordclouds = visualizer.create_class_wordclouds(\n",
    "    quote_texts, quote_clusters, title_prefix=\"Quote Cluster\"\n",
    ")\n",
    "\n",
    "# Display word clouds\n",
    "for cluster, fig in quote_wordclouds.items():\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(fig.axes[0].images[0].get_array())\n",
    "    plt.title(f\"Word Cloud for Quote Cluster {cluster}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Comparing Clustering and Classification Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare clustering and classification for books\n",
    "book_categories = [book['category'] for book in processed_books]\n",
    "\n",
    "# Calculate purity score for book clusters\n",
    "book_purity = book_clustering.calculate_purity(book_categories)\n",
    "print(f\"Book clustering purity score: {book_purity:.3f}\")\n",
    "\n",
    "# Calculate Rand Index for book clusters\n",
    "book_rand_index = book_clustering.calculate_rand_index(book_categories)\n",
    "print(f\"Book clustering Rand Index: {book_rand_index:.3f}\")\n",
    "\n",
    "# Compare with classification metrics\n",
    "print(\"\\nBook classification metrics:\")\n",
    "for metric_name, metric_value in test_metrics.items():\n",
    "    print(f\"  {metric_name}: {metric_value:.3f}\")\n",
    "\n",
    "# Create a contingency table to visualize the relationship between clusters and categories\n",
    "contingency_df = pd.DataFrame({\n",
    "    'Category': book_categories,\n",
    "    'Cluster': book_clusters\n",
    "})\n",
    "\n",
    "# Create a cross-tabulation\n",
    "cross_tab = pd.crosstab(contingency_df['Category'], contingency_df['Cluster'])\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(cross_tab, annot=True, cmap='Blues', fmt='d')\n",
    "plt.title('Relationship between Book Categories and Clusters')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Saving Results\n",
    "\n",
    "Finally, let's save the results to CSV files for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cluster labels to books data\n",
    "for i, book in enumerate(processed_books):\n",
    "    book['cluster'] = int(book_clusters[i])\n",
    "\n",
    "# Add cluster labels to quotes data\n",
    "for i, quote in enumerate(processed_quotes):\n",
    "    quote['cluster'] = int(quote_clusters[i])\n",
    "\n",
    "# Convert to DataFrames\n",
    "books_df = pd.DataFrame(processed_books)\n",
    "quotes_df = pd.DataFrame(processed_quotes)\n",
    "\n",
    "# Save to CSV\n",
    "books_df.to_csv(\"results/books_results.csv\", index=False)\n",
    "quotes_df.to_csv(\"results/quotes_results.csv\", index=False)\n",
    "\n",
    "print(\"Results saved successfully.\")\n",
    "\n",
    "# Display sample of the saved data\n",
    "print(\"\\nSample of books results:\")\n",
    "print(books_df[['title', 'category', 'cluster']].head())\n",
    "\n",
    "print(\"\\nSample of quotes results:\")\n",
    "print(quotes_df[['text', 'author', 'tags', 'cluster']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "In this notebook, we demonstrated the information retrieval project that involved:\n",
    "\n",
    "1. **Web Scraping**: We scraped data from books.toscrape.com and quotes.toscrape.com.\n",
    "2. **Data Preprocessing**: We cleaned and standardized the text data.\n",
    "3. **Clustering**: We grouped similar books and quotes together using K-means clustering.\n",
    "4. **Classification**: We categorized books and quotes based on their content.\n",
    "5. **Evaluation**: We evaluated the performance of our clustering and classification models.\n",
    "6. **Visualization**: We created word clouds and other visualizations to better understand the data.\n",
    "\n",
    "The project demonstrates how information retrieval techniques can be used to organize and analyze unstructured text data from the web."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 }
}